分布式共识要解决的问题是，当一个进程或者多个进程提出一个提议，系统中其他进程能对这个提议达成一致的意见。

Paxos的实现需要遵循两个原则（参考：Paxos Made Simple）

（1）安全原则---保证不能做错的事

1. 只能有一个值被批准，不能出现第二个值把第一个覆盖的情况

2. 每个节点只能学习到已经被批准的值，不能学习没有被批准的值

（2）存活原则---只要有多数服务器存活并且彼此间可以通信，则可以完成决策

1. 最终会批准某个被提议的值

2. 一个值被批准了，其他服务器最终会学习到这个值

（跟ACP的原则或多或少有点相似）

# 1 模型定义

在讨论前，先定义界定下讨论问题的范围。（与ACP相比，修改了部分措辞，大致上是一致的）

（1）计算模型

分布式系统运行在一组进程上，进程可以运行于相同或不同的节点，节点间通过网络交互数据，执行结果最终存储到各个进程所在节点的本次磁盘空间中。

（2）开销模型

算法开销主要关注一下几个点：

- 一次算法执行的节点间交互消息数量（Round Trip）

- 磁盘同步IO次数和延迟

（3）故障模型

根据FLP定理（**没有任何算法可以在存在任何故障的异步系统中确保达到共识）**，我们下面讨论的网络交互都是基于异步模型（相对于同步模型，同步模型下可以对消息传递时间和应答时间设置一个期望上限，通过超时来检测故障），很多时候都无法预估对端的响应时间，所以在生产中，需要设计一个健壮且有实现意义的一致性算法，而非一个可以完美被证明的算法；或者是考虑部分同步模型，通过故障检测、随机化等手段规避异步模型，从而降低故障的概率（不在本文讨论范围了）。

*** TODO：FLP证明、故障检测安排一下，参考下《分布式系统：概念与设计》里面有相关论文 ***

计算模型决定着故障模型，关注的故障类型主要有：

- 消息可能丢失、重复、乱序，但不存在不正确被篡改的消息

- 一个执行失败的节点会停止动作（故障-停止模型：fail-stop），而不会对外发送错误消息（非随机故障模型：Byzantine）

- 节点宕机，为了使节点宕机恢复后不会丢失本地状态，实现的时候还需要考虑将自身的数据、上下文信息持久化到本地磁盘

# 2 Basic Paxos

## 2.1 协议简介

Basic Paxos 是一个基于日志复制状态机实现的，抢占式（指的是propose id占位）的共识算法。

1. 角色
   
   Paxos系统有3中角色：提议者（proposer）、跟随者（acceptor）、学习者（learner）
   
   提议者：接收客户端的请求，并生成提议在系统中表决，获得多数派赞成票后，提议被通过，然后应答客户端成功；
   
   跟随者：对提议者发出的propose进行表决，对于不满足全序要求的propose（或者log index有冲突）时直接拒绝，否则表决赞成；
   
   学习者：接受由跟随者广播的决议，不参与提议的投票协商过程；
   
   参与投票的只有proposer和acceptor，learner无条件接受已经决出的提案。提议者可以是系统中的任意节点。

2. 共识过程
   
   两个个阶段：prepare阶段、accept阶段
   
   （1）prepare 阶段
   
   （2）accept 阶段
   
   一次Write Normal Case（没有故障下的共识过程）
   
   上图就是描述了这么一个过程：
   
   ![](https://km.sankuai.com/api/file/cdn/1278786881/1553940470?contentType=1&isNewContent=false&isNewContent=false)

3. 日志复制状态机
   
   状态机的每个副本上都保存有完全相同的操作日志，保证所有副本状态机按照相同的顺序执行操作，这样由于状态机是确定性的，则一定会得到相同的状态。
   
   **如果状态机的任何一个副本在本地状态机上执行了一个操作，则绝对不会有别的副本在操作序列相同位置**（log index）**执行一个不同的操作（但是存在部分节点上，这个位置的日志还没有被同步，为空）。**
   
   如上图所示，虽然提案已经达成共识，但S3可能因为网络问题没有接收到数据，那么如果要读出这个数据，如果简单的读取单节点的话，读到S3可能获取不到数据或者获取到旧值。
   
   因此，在读取时，其实还要走一遍Paxos，获取到多数派共识后的数据，返回客户端。
   
   （1）写一致性：共识过程完成了写一致性
   
   （2）读一致性：读一致要基于写一致的基础上完成，如果要确定状态机最新状态，需要从一个初始状态，通过重做操作日志，来获得最新状态，而在执行操作日志的时候，对于没有个操作都需要做一次多数派读来决定此次操作已经为系统所共识。
   
   假如系统在运行一段时间后，各个server上的操作日志记录如上图，这时候如果要查询状态机最新状态，则从log index为1开始重新进行Paxos确认系统已经达成共识的日志，再结合初始值逐一应用到状态机中，最终可以得到V2状态。
   
   每次都应用一遍操作日志会带来较高的开销，这时候就需要引入confirm日志（原理就是把已经达成共识的日志应用到状态机，应用之后相当于新的状态机初始值为V2，然后把V2前的日志截断掉，这样也解决了节点日志GC的问题）
   
   因日复制日志之后，共识过程其实增加了一个阶段，确认log index阶段：
   
   propose id大的提议可以先被共识，并持久化吗？
   
   当然可以，在回放日志的时候，是按照log index逻辑顺序回放，并非磁盘写入顺序（具体实现可以参考数据库分槽页 slotted page的设计）

4. 故障恢复
   
   与2PC的故障恢复不同，Paxos可以重新执行Paxos共识流程，来恢复数据（也可以说复制日志相当于2PC中的WAL）
   
   1. proposer收到多数派结果后，发送accept命令前，同步记录一次日志；
   
   2. acceptor在收到proposer的accept命令后，执行前，同步记录一次日志；

  《Paxos made simple》一文中，可以对同步IO进行优化，具体方案如下：

  （1）方案一

1. proposer维护一个提议窗口（假设至少一个窗口结束前proposer都不变），窗口内的各个提议互不影响，可以独立达成共识；

2. 当窗口内所有提议都达成共识后，将这一批的提议持久化；

3. 如果窗口内存在未决的提议，不能开始下一个窗口的提案；
   
   这个方法有点类似组提交的实现，就是把IO合并一起持久化，但这样会带来日志gap的问题。
   
   这个时候S1作为proposer，它宕机了，那么当其他server当上proposer时，这个时候日志gap就产生了，新proposer需要先把未完成的提议，通过Paxos重新共识后才能产生新的提议。
   
   论文里给出一个简单快捷的方法，既然没有达到共识，那我就不作为即可（相当于2PC的默认动作abort一样），我填入no-op操作即可快速恢复服务。

  （2）方案二

  同样是减少IO，我们可以让客户端聚集一批变更后，通过一个提议进行变更，这样对Paxos的修改最少，但是客户端的实现上需要处理（1）并发冲突（2）批量执行，而且这一批的变更是拥有相同的propose id（无法定序），实现复杂度从服务端迁移到客户端。

  具体可见2.2小节。

paxos角色

normal case operation

1. 为什么要有提案号？因为消息可能乱序、重复，提案号用于全序定序用

2. 为什么要多数派？

一致性达成：

1. propose id生成规则（全局递增）：ts 拼 server id，或者 proposer内部记录编号，一共n个节点，

2. 怎么读？走一次Paxos协议（优化：confirm日志标识提案已经通过，也方便acceptor确认清理propose上下文信息，没有confirm日志的则需要重确认）

基于日志复制状态机：状态机的每个副本上都保存有完全相同的操作日志，保证所有副本状态机按照相同的顺序执行操作，这样由于状态机是确定性的，则一定会得到相同的状态。**如果状态机的任何一个副本在本地状态机上执行了一个操作，则绝对不会有别的副本在操作序列相同位置执行一个不同的操作（但日志相同的位置上，部分节点重确认确实日志的时候，可以对没有达成共识的提案可以填充no-op空操作）。**

## 2.2 故障分析

为什么会出现未决提案（丢失了一个commit节点，不能确定提案是不是占多数派赞成）

未决提案恢复分析（最大commit原则，以大的propose id来决定，最终可能达成不同的结果，但不影响一致性原则，paxos made simple里提到一种快速手段，就是填充no-op来直接丢弃未决提案来快速恢复）

## 2.3 实践上的问题

活锁（写冲突）

logid冲突问题

## 2.4 协议开销

rpc：3次，io 2次本地同步io

# 3 Multi Paxos

与Basic Paxos的区别

1. 如何选主（一次Basic Paxos+一次prepare锁定）

2. lease 认证机制

存在问题

1. 幽灵日志（客户端请求后超时未应答，这时候客户端两次查询可能获得不同的结果（两次查询中间没有写入），导致这个问题原因有两：1.未应答客户端表名共识未决，客户端应该重确定，直到成功后才能查询；2.paxos恢复时重确认导致了结果变化）
   
   解决方案：新leader选出后，占用目前最大共识log index+1的日志位置，写入更大的epoch或者term表明新leader接管，这样即使冲确认，旧leader日志也比这个日志位置的消息要旧，被丢弃。

# 4 Fast Paxos

*** TODO：待补充 ***

在了解了Paxos的设计后，Raft的设计应该更容易理解。
