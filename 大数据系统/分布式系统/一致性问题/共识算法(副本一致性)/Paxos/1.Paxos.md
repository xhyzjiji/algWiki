分布式共识要解决的问题是，当一个进程或者多个进程提出一个提议，系统中其他进程能对这个提议达成一致的意见。

> Paxos的实现需要遵循两个原则（参考：Paxos Made Simple）
> 
> （1）安全原则---保证不能做错的事
> 
> 1. 只能有一个值被批准，不能出现第二个值把第一个覆盖的情况
> 
> 2. 每个节点只能学习到已经被批准的值，不能学习没有被批准的值
> 
> （2）存活原则---只要有多数服务器存活并且彼此间可以通信，则可以完成决策
> 
> 1. 最终会批准某个被提议的值
> 
> 2. 一个值被批准了，其他服务器最终会学习到这个值
> 
> （跟ACP的原则或多或少有点相似）

# 1 模型定义

在讨论前，先定义界定下讨论问题的范围。（与ACP相比，修改了部分措辞，大致上是一致的）

（1）计算模型

分布式系统运行在一组进程上，进程可以运行于相同或不同的节点，节点间通过网络交互数据，执行结果最终存储到各个进程所在节点的本次磁盘空间中。

（2）开销模型

算法开销主要关注一下几个点：

- 一次算法执行的节点间交互消息数量（Round Trip）

- 磁盘同步IO次数和延迟

（3）故障模型

根据FLP定理（**没有任何算法可以在存在任何故障的异步系统中确保达到共识）**，我们下面讨论的网络交互都是基于异步模型（相对于同步模型，同步模型下可以对消息传递时间和应答时间设置一个期望上限，通过超时来检测故障），很多时候都无法预估对端的响应时间，所以在生产中，需要设计一个健壮且有实现意义的一致性算法，而非一个可以完美被证明的算法；或者是考虑部分同步模型，通过故障检测、随机化等手段规避异步模型，从而降低故障的概率（不在本文讨论范围了）。

*** TODO：FLP证明、故障检测安排一下，参考下《分布式系统：概念与设计》里面有相关论文 ***

计算模型决定着故障模型，关注的故障类型主要有：

- 消息可能丢失、重复、乱序，但不存在不正确被篡改的消息

- 一个执行失败的节点会停止动作（故障-停止模型：fail-stop），而不会对外发送错误消息（非随机故障模型：Byzantine）

- 节点宕机，为了使节点宕机恢复后不会丢失本地状态，实现的时候还需要考虑将自身的数据、上下文信息持久化到本地磁盘

# 2 Basic Paxos

## 2.1 协议简介

Basic Paxos 是一个基于日志复制状态机实现的，抢占式（指的是propose id占位）的共识算法。

1. 角色
   
   Paxos系统有3中角色：提议者（proposer）、接受者（acceptor）、学习者（learner）
   
   提议者：接收客户端的请求，并生成提议在系统中表决，获得多数派赞成票后，提议被通过，然后应答客户端成功；
   
   接受者：对提议者发出的propose进行表决，对于不满足全序要求的propose（或者log index有冲突）时直接拒绝，否则表决赞成；
   
   学习者：接受由接受者广播的决议，不参与提议的投票协商过程；
   
   参与投票的只有proposer和acceptor，learner无条件接受已经决出的提案。proposer可以是系统中的任意节点，而且一般proposer自己本身也是一个acceptor。

2. 共识过程
   
   两个个阶段：prepare阶段、accept阶段
   
   （1）prepare 阶段
   
   （2）accept 阶段
   
   一次Write Normal Case（没有故障下的共识过程）
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_progress.png)
   
   上图就是描述了这么一个过程：
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/1553940469.jpeg)
   
   > :question: 为什么Acceptor要求 Prepare 阶段 n > minProposal，而 Accept 阶段要求 n >= minProposal？
   > 
   > 1. Accept 阶段肯定是不可以要求 n > minProposal，因为Prepare阶段已经把本地的minProposal更新成请求的proposal，所以Accept阶段需要接受 = minProposal，否则就出现拒绝自己Prepare的乌龙；Accept阶段还要接受 > minProposal，因为可能存在Prepare请求被新的proposer抢占，Acceptor不应该答应旧的propose。
   > 
   > 2. Prepare 阶段要求请求的propose id（就是n）比本地的大，这个是需要解决下面说到的，同一个ballot中可能有多个proposer同时提议，或者是先后有proposer提议并且消息有乱序，propose id其实就是一个对请求定序的过程，并且是Last Write Win。
   
   > :leaves: （1）为什么需要propose id？如果proposer发什么acceptor接受什么，会有什么问题？
   > 
   > 原因主要是：
   > 
   > 1. 消息可能乱序到达；
   >    
   >    假设没有propose id，消息无法定序，acceptor无脑接受proposer提议值
   >    
   >    最终V1获得多数节点共识，先到的提议把后来的提议覆盖了。
   > 
   > 2. 同一个log index日志一旦达成共识不应被覆盖；
   >    
   >    同一时间，完成了多个propose共识
   >    
   >    于是，操作日志某一个log index中，有一段时间是V1，然后突然变为了V2。
   > 
   > :leaves: （2）如何生成一个全局递增的propose id？
   > 
   > 1. 首先，从acceptor中返回的reject信息，可以得出接受过的最大propose id（假设提交失败了）
   > 
   > 2. 然后根据一定规则生成一个比（a）中获得的最大propose id更大的值即可
   >    
   >    1. propose id = （timestamp << k) | serverId
   >    
   >    2. propose id = n * m + serverId （serverId从0...n-1编码，共n个节点，m为server内部记录的递增序号，只需要每次根据最大propose id计算序号就好）
   
   > 为什么需要多数派一致的应答结果？
   > 
   > ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_1.png)
   > 
   > 各个proposer各自提议，系统一致性荡然无存。

3. 日志复制状态机
   
   状态机的每个副本上都保存有<font color='red'>完全相同的操作日志</font>，保证所有副本状态机按照<font color='red'>相同的顺序</font>执行操作，这样由于状态机是确定性的，则一定会得到<font color='red'>相同的状态</font>。
   
   **如果状态机的任何一个副本在本地状态机上执行了一个操作，则绝对不会有别的副本在操作序列相同位置**（Paxos里叫Ballot，Raft里叫log index，我们都叫log index好了）**执行一个不同的操作（但是存在部分节点上，这个位置的日志还没有被同步，为空）。**
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_2.png)
   
   如上图所示，虽然提案已经达成共识，但S3可能因为网络问题没有接收到数据，那么如果要读出这个数据，如果简单的读取单节点的话，读到S3可能获取不到数据或者获取到旧值。
   
   因此，在读取时，其实还要走一遍Paxos，获取到多数派共识后的数据，返回客户端。
   
   （1）写一致性：共识过程完成了写一致性
   
   （2）读一致性：读一致要基于写一致的基础上完成，如果要确定状态机最新状态，需要从一个初始状态，通过重做操作日志，来获得最新状态，而在执行操作日志的时候，对于没有个操作都需要做一次多数派读来决定此次操作已经为系统所共识。
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_3.png)
   
   假如系统在运行一段时间后，各个server上的操作日志记录如上图，这时候如果要查询状态机最新状态，则从log index为1开始重新进行Paxos确认系统已经达成共识的日志，再结合初始值逐一应用到状态机中，最终可以得到V2状态。
   
   > :warning: 每次都应用一遍操作日志会带来较高的开销，这时候就需要引入confirm日志（原理就是把已经达成共识的日志应用到状态机，应用之后相当于新的状态机初始值为V2，然后把V2前的日志截断掉，这样也解决了节点日志GC的问题）
   
   共识过程其实增加了一个“getLogIndex”阶段，来确认最大的（可写入的）log index：
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_4.png)
   
   Proposer获取LogIndex时，只需要满足多数节点返回的LogIndex取最大值即可（因为多数节点的最大LogIndex必定包含达成共识的日志）
   
   > :question: log index更大的提议可以先被共识，并持久化吗？
   > 
   > 当然可以，在回放日志的时候，是按照log index逻辑顺序回放，并非磁盘写入顺序（联想一下数据库分槽页 slotted page的设计，为什么连续的id可以以乱序的形式顺序写入文件中）

4. 故障恢复
   
   与2PC的故障恢复不同，Paxos可以重新执行Paxos共识流程，来恢复数据（也可以说复制日志相当于2PC中的WAL）
   
   1. proposer收到多数派结果后，发送accept命令前，同步记录一次日志；
   
   2. acceptor在收到proposer的accept命令后，执行前，同步记录一次日志；
   
   > 《Paxos made simple》一文中，可以对同步IO进行优化，具体方案如下：
   > 
   > （1）方案一
   > 
   > 1. proposer维护一个提议窗口（假设至少一个窗口结束前proposer都不变），窗口内的各个提议互不影响，可以独立达成共识；
   > 
   > 2. 当窗口内所有提议都达成共识后，将这一批的提议持久化；
   > 
   > 3. 如果窗口内存在未决的提议，不能开始下一个窗口的提案；
   >    
   >    这个方法有点类似组提交的实现，就是把IO合并一起持久化，但这样会带来<font color='red'>日志gap</font>的问题。
   >    
   >    ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_poxis_gap.png)
   >    
   >    这个时候S1作为proposer，它宕机了，那么当其他server当上proposer时，这个时候日志gap就产生了，新proposer需要先把未完成的提议，通过Paxos重新共识后才能产生新的提议。
   >    
   >    论文里给出一个简单快捷的方法，既然没有达到共识，那我就不作为即可（相当于2PC的默认动作abort一样），我填入no-op操作即可快速恢复服务。
   >    
   >    ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_gap2.png)
   >    
   >    :warning: Paxos支持日志gap，但Raft则不支持，到Raft篇我们再看看具体原因是什么。
   > 
   > （2）方案二
   > 
   > 同样是减少IO，我们可以让客户端聚集一批变更后，通过一个提议进行变更，这样对Paxos的修改最少，但是客户端的实现上需要处理（1）并发冲突（2）批量执行，而且这一批的变更是拥有相同的propose id（无法定序），实现复杂度从服务端迁移到客户端。
   
   具体可见2.2小节。

## 2.2 故障分析

当共识流程没有完成时，proposer节点故障，这个时候无法确认这个提议到底是commit还是uncommit（commit == 达成共识）

比如：

![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_recovery.png)

从上帝视角，我们知道V1和V2是已经commit的，但是当S2宕机后，V2从原来的多数派，变成个不过半的少数派，这个时候无法确认V2是否已经commit过了。

![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_recovery2.png)

（1）可能的场景1：V2未commit

![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_commit.png)

（2）可能的场景2：V2已经commit

![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_commit2.png)

当故障发生后，proposer需要对未对齐的日志进行冲确认（reconfigure）——重新走Paxos流程，并且以“最大commit原则”——以最大的propose id的提案为最终提案进行确认。

> :leaves: 对于每条日志的重确认，需要执行一轮完整的Paxos过程，可能有些日志在恢复前确实未形成多数派备份，需要通过重新执行Paxos来把这些日志重新持久化才能回放。这种不管日志是否曾经形成多数派备份，都重新尝试持久化的原则，我们称之为“最大commit原则”。之所以要遵守“最大commit原则”，是因为我们无法区分出来未形成多数派备份的日志，而这些日志在上一任leader任期内，也必然是“未决”状态，尚未应答客户端，所以无论如何都重新持久化都是安全的。

> 实际的故障恢复场景：
> 
> （1）一个未提交日志的故障恢复
> 
> ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/1554219370.png)
> 
> - 注：这里的水平表示集群个节点在某一时刻的状态，垂直线表示时间线。
> 
> （2）一个已提交的日志故障恢复过程：
> 
> 同上述过程一致，也是重新进行paxos，但已提交的日志是否存在被覆盖的可能？
> 
> ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_recovery3.png)
> 
> ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/basic_paxos_recovery4.png)

## 2.3 工程问题

（1）活锁问题

![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_live_lock.png)

因为Basic Paxos支持多个proposer同时提交提案，极端情况下，在上一个提议prepare后，立即被下一个提议prepare（更大的propose id）抢占提交权限，等到accept阶段被拒绝后，重新生成新的propose id，周而复此，导致请求无法永远无法被提交。

（2）propose id 和 log index冲突问题

因为Basic Paxos支持多个proposer同时提交提案，使得并发较高的情况下，propose id（就是上面的活锁）和log index冲突率上升，一旦log index冲突，还需要重新获取log index。

## 2.4 协议开销

- 消息开销：三阶段的RoundTrip，即3次RPC调用

- 磁盘IO开销：2次本地同步IO

# 3 Multi Paxos

## 3.1 优化

为了弥补Basic Paxos的不足，Multi Paxos在Basic Paxos的基础上进行改良：

Multi Paxos会首先选出一个Leader作为proposer，这样的好处有：

1. 由于只有一个proposer，所有提议都可以在proposer内部进行全局定序，降低id冲突的可能性

2. 由于不存在不同proposer并发发出提议，活锁问题得到解决

3. 由于不存在多个propose抢占的情况，在leader任期内，可以不进行prepare，其他节点可以无条件信任leader，二阶段变为一阶段（只有accept阶段）

（1）Leader选举

Leader选举可以通过Basic Paxos，在集群中得到一个Leader的共识。

但是要注意下面的情况：

![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/multi_paxos_leader.png)

由于Basic Paxos允许不同的server发起propose，当出现上面的场景时，S1和S3会同时认为Leader选举得到共识，同时成为Leader。

根据Paxos原则，达到共识的提议无法被否决，即使S1的Leader身份已经无效，而且任期内需要继续使用对应的提议号进行，这时候S1的提议也不再被大多数节点接受。

![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/multi_paxos_leader2.png)

解决思路：可以以Leader身份在执行一次Paxos，以确认自己的Leader身份已经生效。

> :question: 当出现acceptor拒绝提议时，S1 是否可以退出Leader，询问集群新Leader身份，或者等待新一轮的Leader？

（2）Lease问题

在Leader任期内，acceptor无条件接受提议，但是，当Leader因高负载导致响应超时后，被集群其他节点误判为节点故障，触发新的一轮Leader选举；

当新Leader被选出后，旧Leader负载恢复并继续执行上一次的请求，这个时候相当于有两个Leader都拥有了决策的权限。

增加Lease机制，要求acceptor<font color='red'>必须主动检查Leader的任期信息</font>，当acceptor发现Leader过期时，则拒绝这次的请求。

Lease机制类似于路由器的续约机制，只要与acceptor维持身份认证（certification），就可以通过提议。

Leade机制同时会拉长Leader故障的时候切换所需的时间，因为acceptor需要等待上一个Lease过期后，才能接受新的Lease。

## 3.2 工程问题

1. 幽灵复现
   
   这个问题是指部分提议在共识过程中，由于节点故障到恢复之后，客户端不同时期的查询结果可能不同。
   
   （1）初始状态，S1收到1-10的请求，但只有1-5号日志被提交，6-10号日志客户端得到服务端响应超时
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_ghost_log.png)
   
   （2）此时S1宕机，S2被选为Leader，因为S2和S3最大log index是5，那么S2从log index=6开始追加日志，这个时候客户端请求了6-20，Paxos支持空洞，所以，可能只有6和20号日志被提交
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_ghost_log2.png)
   
   （3）此时S2宕机，S1恢复并当选Leader，并对未确认的日志进行重确认
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_ghost_log3.png)
   
   重确认后将未决的日志进行修正
   
   ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_ghost_log4.png)
   
   在（1）（2）时期，客户端都是查询不到数据的（P1和P2两轮的数据都未提交），（时期（2）中，客户端发送了新的请求）但时期（3）的时候，7-10号旧日志（P1任期内的日志）却被应用到状态机上，11-19号日志nop对提交结果没有影响
   
   > 解决方案是，在新Leader产生后，需要先提交一条新的Leader生效日志，标识之后的任期内的有序性，当发现某个log index的位置后出现Epoch（图片中的P1、P2、P3）比之前的小，回放时忽略这类日志。
   > 
   > ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_ghost_log5.png)
   > 
   > ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_ghost_log6.png)
   > 
   > P2,6 是新Leader当选消息。
   > 
   > ![](/Users/panyongfeng/Documents/basic_framework/wiki/大数据系统/分布式系统/一致性问题/共识算法(副本一致性)/pics/paxos_ghost_log7.png)
   > 
   > 不过归根结底，其实客户端都没有收到7-10号日志的提交响应，这个时候客户端没有重确认就立即查询，请求在服务端可能是以下几种状态：（1）没有执行，请求丢失（2）请求没有丢失，故障恢复后执行或者回滚（3）请求执行中（半事务状态）（4）请求已经被执行。
   > 
   > > :leaves: 如果新Leader当选之后，可以立即广播一条当选的日志，来分割不同任期（即使没有新的提议发生，这个广播也能标识日志），并且方便在重确认后，回放时不会把过早任期的数据回放到状态机上。
   > 
   > **注：Raft对待已提交的日志同样存在这样的问题，不过因为协议约束条件不同(日志空洞)，解决方法也有所区别**

2. 很难理解和证明可靠性（很难理解如何科学证明，就算解决了上面所说的所有问题，也不能完全证明这个算法可靠，即使经过TLA验证，经过实现可能就会存在不同程度的问题，或者不同的故障叠加出现的时候是否会有其他问题等等）

# 4 Fast Paxos

*** TODO：待补充 ***

# 附：Basic Paxos的数学证明

https://zhuanlan.zhihu.com/p/21438357

严谨证明看懂之后再分享。

在了解了Paxos的设计后，Raft针对Paxos的简化应该更容易理解。

参考文献：

1. 2001-Paxos Made Simple.pdf

2. PaxosRaft 分布式一致性算法原理剖析及其在实战中的应用.pdf

3. https://zhuanlan.zhihu.com/p/40175038

4. https://zhuanlan.zhihu.com/p/20417442

5. 2005-Fast Paxos.pdf
